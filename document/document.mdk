Title         : HW1 Data Mining
Author        : Baqiao Liu
Logo          : True

[TITLE]

# Problem 1

## a

FP-Growth's running time scales linearly w.r.t. the number of itemsets, while Apriori scales exponentially.
FP-Growth has no explicit candidate generation, while Apriori explicitly constructs candidates by using
the frequent itemsets of the previous scan and checking if they overlap but by one item.

## b

By contradiction, fix some minimum support $s$ and a frequent itemset $S$ s.t. some nonempty subset $T \subseteq S$
has lower support than that of $S$. WLOG $|T| = |S| - 1$, or else consider any subset of $S$ and superset of $T$ that differs from $S$ by
cardinality of 1; call it $S'$. If support of $S'$ satisfies $sup(S') < sup(S)$, then $S' \subset S$ are the
two sets we want for our assumption. Otherwise we have $sup(T) < sup(S) \leq sup(S')$, and we can recurse on
this process until we obtain a $|S'| - 1 = |T|$ which satisfies our assumption. Thus $|T| = |S| - 1$, and let
$e$ be s.t. $T \cup \{e\} = S$. Let the itemsets be called $I$, for each $I_i$,
$S \subseteq I_i \implies (T \cup \{e\}) \subseteq I_i \implies T \subseteq I_i \land \{e\} \subseteq I_i$, which
implies that any itemset in which $S$ occurs also must have $T$ occuring, which implies that the support of
$T$ is at least the support of $S$. This proves (2).

For (1), by contradiction suppose that some non-empty subset of a frequent itemset is not frequent. Then
by definition this contracts (2).

## c

They are not null-invariant. Consider when evaluating the Lift of A -> B, and we fix all the
non-null events (i.e. A or B occurs), but vary the # of null events (neither A or B occurs); then
these two measures change -- this might not be desirable for some settings.


![](https://3.bp.blogspot.com/-0Xec064LuNE/VPZnPzq6s6I/AAAAAAAAB4s/fCGgdTejutU/s1600/AB%2BVenn%2BDiagram.jpg)
<!-- end merge -->

The Jacaard measure uses only the cardinality of the sets in the non-gray region. The
measure is invariant of the size of the gray region (the region containig the null records):
it is null-invariant.

## d

# Problem 2

## a

```
Scan 1
b 3 => ✓ keep
a 4 => ✓ keep
o 1 => ✘ not frequent enough
l 2 => ✘ not frequent enough
p 3 => ✓ keep
m 3 => ✓ keep
f 4 => ✓ keep
d 2 => ✘ not frequent enough
h 2 => ✘ not frequent enough
g 1 => ✘ not frequent enough
c 5 => ✓ keep
Scan 2
p, c 3 => ✓ keep
c, f 3 => ✓ keep
b, f 1 => ✘ not frequent enough
p, b 1 => ✘ not frequent enough
p, f 2 => ✘ not frequent enough
p, m 2 => ✘ not frequent enough
f, m 2 => ✘ not frequent enough
c, a 4 => ✓ keep
c, m 3 => ✓ keep
b, m 1 => ✘ not frequent enough
b, a 2 => ✘ not frequent enough
f, a 2 => ✘ not frequent enough
b, c 2 => ✘ not frequent enough
p, a 3 => ✓ keep
a, m 3 => ✓ keep
Scan 3
a, c, f 2 => ✘ not frequent enough
a, p, c 3 => ✓ keep
p, c, m 2 => ✘ not frequent enough
c, f, m 2 => ✘ not frequent enough
f, p, c 2 => ✘ not frequent enough
c, a, m 3 => ✓ keep
p, a, m 2 => ✘ not frequent enough
```

Final derived frequent itemsets:

```
c
a
c, a
f
c, f
p, c
b
a, p, c
p, a
c, m
p
a, m
c, a, m
m
```


## b

![IMG_F95DB66796B4-1]

[IMG_F95DB66796B4-1]: images/IMG_F95DB66796B4-1.JPEG "IMG_F95DB66796B4-1" { width:auto; max-width:90% }

## c

```
A -> C
O -> L
O -> F
```

# Problem 3

## a
 - product types: 169
 - total transactions: 9835
 - average product per transaction: 11.05
 - 5 most popular product: whole milk, other vegetables, rolls/buns, soda, yogurt
 - 5 least popular product: baby food, sound storage medium, preservation products, bags, kitchen utensil

## b

`min_support` was set to 500.

Top-10:

```
[(frozenset({'whole milk'}), 2513),
 (frozenset({'other vegetables'}), 1903),
 (frozenset({'rolls/buns'}), 1809),
 (frozenset({'soda'}), 1715),
 (frozenset({'yogurt'}), 1372),
 (frozenset({'bottled water'}), 1087),
 (frozenset({'root vegetables'}), 1072),
 (frozenset({'tropical fruit'}), 1032),
 (frozenset({'shopping bags'}), 969),
 (frozenset({'sausage'}), 924)]
```

|             | tropical fruit | not tropical fruit |      |
|-------------|----------------|--------------------|------|
| sausage     | 137            | 895                | 1032 |
| not sausage | 787            | 8016               | 8803 |
|             | 924            | 8911               |      |

`lift(tropical fruit -> sausage)` = 1.4130035823349778

## c
```
Itemset size => running time
1000 => fp-growth: 0.018999699999767472, apriori: 0.10236879999865778
2000 => fp-growth: 0.0456969000006211, apriori: 0.23432859999957145
3000 => fp-growth: 0.10826420000012149, apriori: 0.3962135000001581
4000 => fp-growth: 0.16897269999935816, apriori: 0.5983805999985634
5000 => fp-growth: 0.26302639999994426, apriori: 0.970965200000137
```

It can be seen that apriori is obviuosly less scalable compared to fp-growth.
It looks like exponential time already even with such small datasets while
fp-growth clearly scales better.


# Problem 5

## a

See (b). By closure properties of valid kernels, if two kernels are semi-definite
and summed (taken the product) together, then their sum/product is also semi-definite, i.e. eigenvalues
non-negative. Thus is RBF kernel is the sum of infinite number of polynomial
kernels

## b

WLOG, we assume the gamma variable to be 0.5.

## c

## d

This SVM would be just like nearest neighbor classifier.


# Problem 6

## a

Classification results (by row):

```
[0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 1
 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0
 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0
 0 1 0 1 0]
```

The accuracy achived is 0.94.


## b

Classification results (by row):

```
[0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
       1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
       0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
       0, 0, 1, 0, 1, 0]
```

The accuracy achived is 100%. 21 features are selected.

# Problem 7

See code for implementation.

Classification results (by row):

```
[0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1
 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0
 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0
 0 1 0 1 0]
```

Accuracy is 0.98.

# Problem 8

## a

From the formula, RWR, compared to the other (single path, shortest path, etc.),
will simutaneously "consider" all of the possibilities going from a starting node
while taking the weights into account at each iteration. Looking at the iteration
formula more closely ($r_{i}=c A r_{i}+(1-c) e_{i}$), we see that the matrix $A$, will do multiple
"choices" by its matrix multiplication. The restart probability also ensures that stochastically
the node will be reset and thus gives more consideration to all the proximity nodes around the starting condition.



## b

The top ten indices (along with the weights associated with each node):
```
[(420, 0.00319884364887129),
 (5, 0.0024865987565811706),
 (160, 0.0024104627178654914),
 (86, 0.002384791146662446),
 (301, 0.0021187365469630304),
 (121, 0.002075908392562482),
 (42, 0.002047447746306882),
 (411, 0.001995249380766448),
 (82, 0.0019756997652009126),
 (932, 0.0019446438319270262)]
```

[reference manual]: http://research.microsoft.com/en-us/um/people/daan/madoko/doc/reference.html  "Madoko reference manual"
